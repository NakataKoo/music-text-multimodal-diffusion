{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('common/get_model'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "module_path = os.path.abspath(os.path.join('../models/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from common.get_model import get_model, register\n",
    "from sd import DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import copy\n",
    "from functools import partial\n",
    "from contextlib import contextmanager\n",
    "\n",
    "version = '0'\n",
    "symbol = 'codi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('codi', version)\n",
    "class CoDi(DDPM):\n",
    "    def __init__(self,\n",
    "                 audioldm_cfg,\n",
    "                 optimus_cfg,\n",
    "                 clap_cfg,\n",
    "                 text_scale_factor=4.3108,\n",
    "                 audio_scale_factor=0.9228,\n",
    "                 scale_by_std=False,\n",
    "                 *args, \n",
    "                 **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        self.audioldm = get_model()(audioldm_cfg)\n",
    "            \n",
    "        self.optimus = get_model()(optimus_cfg)\n",
    "            \n",
    "        self.clap = get_model()(clap_cfg)\n",
    "        \n",
    "        if not scale_by_std:\n",
    "            self.text_scale_factor = text_scale_factor\n",
    "            self.audio_scale_factor = audio_scale_factor\n",
    "        else:\n",
    "            self.register_buffer(\"text_scale_factor\", torch.tensor(text_scale_factor))\n",
    "            self.register_buffer(\"audio_scale_factor\", torch.tensor(audio_scale_factor))\n",
    "\n",
    "        self.freeze()\n",
    "        \n",
    "    def freeze(self):\n",
    "        self.eval()\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def optimus_encode(self, text):\n",
    "        if isinstance(text, List):\n",
    "            tokenizer = self.optimus.tokenizer_encoder\n",
    "            token = [tokenizer.tokenize(sentence.lower()) for sentence in text]\n",
    "            token_id = []\n",
    "            for tokeni in token:\n",
    "                token_sentence = [tokenizer._convert_token_to_id(i) for i in tokeni]\n",
    "                token_sentence = tokenizer.add_special_tokens_single_sentence(token_sentence)\n",
    "                token_id.append(torch.LongTensor(token_sentence))\n",
    "            token_id = torch._C._nn.pad_sequence(token_id, batch_first=True, padding_value=0.0)[:, :512]\n",
    "        else:\n",
    "            token_id = text\n",
    "        z = self.optimus.encoder(token_id, attention_mask=(token_id > 0))[1]\n",
    "        z_mu, z_logvar = self.optimus.encoder.linear(z).chunk(2, -1)\n",
    "        return z_mu.squeeze(1) * self.text_scale_factor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def optimus_decode(self, z, temperature=1.0):\n",
    "        z = 1.0 / self.text_scale_factor * z\n",
    "        return self.optimus.decode(z, temperature)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def audioldm_encode(self, audio, time=2.0):\n",
    "        encoder_posterior = self.audioldm.encode(audio, time=time)\n",
    "        z = encoder_posterior.sample().to(audio.dtype)\n",
    "        return z * self.audio_scale_factor\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def audioldm_decode(self, z):\n",
    "        if (torch.max(torch.abs(z)) > 1e2):\n",
    "            z = torch.clip(z, min=-10, max=10)\n",
    "        z = 1.0 / self.audio_scale_factor * z\n",
    "        return self.audioldm.decode(z)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def mel_spectrogram_to_waveform(self, mel):\n",
    "        # Mel: [bs, 1, t-steps, fbins]\n",
    "        if len(mel.size()) == 4:\n",
    "            mel = mel.squeeze(1)\n",
    "        mel = mel.permute(0, 2, 1)\n",
    "        waveform = self.audioldm.vocoder(mel)\n",
    "        waveform = waveform.cpu().detach().numpy()\n",
    "        return waveform\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def clip_encode_text(self, text, encode_type='encode_text'):\n",
    "        swap_type = self.clip.encode_type\n",
    "        self.clip.encode_type = encode_type\n",
    "        embedding = self.clip.encode(text)\n",
    "        self.clip.encode_type = swap_type\n",
    "        return embedding\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def clap_encode_audio(self, audio):\n",
    "        embedding = self.clap(audio)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, x=None, c=None, noise=None, xtype='image', ctype='prompt', u=None, return_algined_latents=False):\n",
    "        if isinstance(x, list):\n",
    "            t = torch.randint(0, self.num_timesteps, (x[0].shape[0],), device=x[0].device).long()\n",
    "        else:\n",
    "            t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=x.device).long()\n",
    "        return self.p_losses(x, c, t, noise, xtype, ctype, u, return_algined_latents)\n",
    "\n",
    "    def apply_model(self, x_noisy, t, cond, xtype='image', ctype='prompt', u=None, return_algined_latents=False):\n",
    "        return self.model.diffusion_model(x_noisy, t, cond, xtype, ctype, u, return_algined_latents)\n",
    "\n",
    "    def get_pixel_loss(self, pred, target, mean=True):\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = (target - pred).abs()\n",
    "            if mean:\n",
    "                loss = loss.mean()\n",
    "        elif self.loss_type == 'l2':\n",
    "            if mean:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred)\n",
    "            else:\n",
    "                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
    "        else:\n",
    "            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n",
    "        loss = torch.nan_to_num(loss, nan=0.0, posinf=0.0, neginf=-0.0)\n",
    "        return loss\n",
    "\n",
    "    def get_text_loss(self, pred, target):\n",
    "        if self.loss_type == 'l1':\n",
    "            loss = (target - pred).abs()\n",
    "        elif self.loss_type == 'l2':\n",
    "            loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n",
    "        loss = torch.nan_to_num(loss, nan=0.0, posinf=0.0, neginf=0.0)    \n",
    "        return loss\n",
    "\n",
    "    def p_losses(self, x_start, cond, t, noise=None, xtype='image', ctype='prompt', u=None, return_algined_latents=False):\n",
    "        if isinstance(x_start, list):\n",
    "            noise = [torch.randn_like(x_start_i) for x_start_i in x_start] if noise is None else noise\n",
    "            x_noisy = [self.q_sample(x_start=x_start_i, t=t, noise=noise_i) for x_start_i, noise_i in zip(x_start, noise)]\n",
    "            model_output = self.apply_model(x_noisy, t, cond, xtype, ctype, u, return_algined_latents)\n",
    "            if return_algined_latents:\n",
    "                return model_output\n",
    "            \n",
    "            loss_dict = {}\n",
    "\n",
    "            if self.parameterization == \"x0\":\n",
    "                target = x_start\n",
    "            elif self.parameterization == \"eps\":\n",
    "                target = noise\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "            \n",
    "            loss = 0.0\n",
    "            for model_output_i, target_i, xtype_i in zip(model_output, target, xtype):\n",
    "                if xtype_i == 'text':\n",
    "                    loss_simple = self.get_text_loss(model_output_i, target_i).mean([1])\n",
    "                elif xtype_i == 'audio':\n",
    "                    loss_simple = self.get_pixel_loss(model_output_i, target_i, mean=False).mean([1, 2, 3])\n",
    "                loss += loss_simple.mean()\n",
    "            return loss / len(xtype)\n",
    "        \n",
    "        else:    \n",
    "            noise = torch.randn_like(x_start) if noise is None else noise\n",
    "            x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "            model_output = self.apply_model(x_noisy, t, cond, xtype, ctype)\n",
    "\n",
    "            loss_dict = {}\n",
    "\n",
    "            if self.parameterization == \"x0\":\n",
    "                target = x_start\n",
    "            elif self.parameterization == \"eps\":\n",
    "                target = noise\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "            if xtype == 'text':\n",
    "                loss_simple = self.get_text_loss(model_output, target).mean([1])\n",
    "            elif xtype == 'audio':\n",
    "                loss_simple = self.get_pixel_loss(model_output, target, mean=False).mean([1, 2, 3])\n",
    "            loss = loss_simple.mean()\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoDi()のインスタンス化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "def load_yaml_config(filepath):\n",
    "    with open(filepath, 'r') as file:\n",
    "        return yaml.safe_load(file)\n",
    "    \n",
    "class ConfigObject(object):\n",
    "    def __init__(self, dictionary):\n",
    "        for key in dictionary:\n",
    "            setattr(self, key, dictionary[key])\n",
    "\n",
    "audioldm=load_yaml_config(\"../../configs/model/audioldm.yaml\")\n",
    "optimus=load_yaml_config(\"../../configs/model/optimus.yaml\")\n",
    "clap=load_yaml_config(\"../../configs/model/clap.yaml\")\n",
    "unet=load_yaml_config(\"../../configs/model/openai_unet.yaml\")\n",
    "\n",
    "audioldm = ConfigObject(audioldm[\"audioldm_autoencoder\"])\n",
    "optimus = ConfigObject(optimus[\"optimus_vae\"])\n",
    "clap = ConfigObject(clap[\"clap_audio\"])\n",
    "unet = ConfigObject(unet[\"openai_unet_codi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######################\n",
      "# Running in eps mode #\n",
      "#######################\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempted relative import beyond top-level package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m codi \u001b[38;5;241m=\u001b[39m \u001b[43mCoDi\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudioldm_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudioldm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimus_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclap_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munet_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mCoDi.__init__\u001b[0;34m(self, audioldm_cfg, optimus_cfg, clap_cfg, text_scale_factor, audio_scale_factor, scale_by_std, *args, **kwargs)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m      4\u001b[0m              audioldm_cfg,\n\u001b[1;32m      5\u001b[0m              optimus_cfg,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m              \u001b[38;5;241m*\u001b[39margs, \n\u001b[1;32m     11\u001b[0m              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudioldm \u001b[38;5;241m=\u001b[39m get_model()(audioldm_cfg)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimus \u001b[38;5;241m=\u001b[39m get_model()(optimus_cfg)\n",
      "File \u001b[0;32m/raid/m236866/md-mt/core/models/sd.py:95\u001b[0m, in \u001b[0;36mDDPM.__init__\u001b[0;34m(self, unet_config, timesteps, use_ema, beta_schedule, beta_linear_start, beta_linear_end, loss_type, clip_denoised, cosine_s, given_betas, l_simple_weight, original_elbo_weight, v_posterior, parameterization, use_positional_encodings, learn_logvar, logvar_init)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_positional_encodings \u001b[38;5;241m=\u001b[39m use_positional_encodings\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OrderedDict\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(OrderedDict([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiffusion_model\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43munet_config\u001b[49m\u001b[43m)\u001b[49m)]))\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema \u001b[38;5;241m=\u001b[39m use_ema\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_ema:\n",
      "File \u001b[0;32m/raid/m236866/md-mt/core/models/common/get_model.py:63\u001b[0m, in \u001b[0;36mget_model.__call__\u001b[0;34m(self, cfg, verbose)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcodi\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m codi\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopenai_unet\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlatent_diffusion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m diffusion_unet\n\u001b[1;32m     66\u001b[0m args \u001b[38;5;241m=\u001b[39m preprocess_model_args(cfg\u001b[38;5;241m.\u001b[39margs)\n",
      "\u001b[0;31mValueError\u001b[0m: attempted relative import beyond top-level package"
     ]
    }
   ],
   "source": [
    "codi = CoDi(audioldm_cfg=audioldm, optimus_cfg=optimus, clap_cfg=clap, unet_config=unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
