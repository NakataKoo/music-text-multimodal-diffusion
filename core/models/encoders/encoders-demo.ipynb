{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../common/get_model'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "\n",
    "from clap_modules.open_clip import create_model\n",
    "from clap_modules.training.data import get_audio_features\n",
    "\n",
    "from common.get_model import register "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@register('clap_audio')\n",
    "class CLAPAudioEmbeddingClassifierFreev2(nn.Module):\n",
    "    \"\"\"Uses the CLAP audio encoder\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_path=\"\",\n",
    "        key=\"waveform\",\n",
    "        sampling_rate=16000,\n",
    "        embed_mode=\"audio\",\n",
    "        unconditional_prob=0.1,\n",
    "        random_mute=False,\n",
    "        max_random_mute_portion=0.5,\n",
    "        training_mode=True,\n",
    "        joint_embed_shape=768,\n",
    "        embed_shape=512,\n",
    "        num_layers=12,\n",
    "        depths=[2, 2, 6, 2],\n",
    "        amodel=\"HTSAT-large\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.key = key\n",
    "        self.amodel = amodel  # or 'PANN-14'\n",
    "        self.tmodel = \"roberta\"  # the best text encoder in our training\n",
    "        self.enable_fusion = False  # False if you do not want to use the fusion model\n",
    "        self.fusion_type = \"aff_2d\"\n",
    "        self.pretrained = pretrained_path\n",
    "        self.embed_mode = embed_mode\n",
    "        self.embed_mode_orig = embed_mode\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.unconditional_prob = unconditional_prob\n",
    "        self.random_mute = random_mute\n",
    "        self.joint_embed_shape = joint_embed_shape\n",
    "        self.max_random_mute_portion = max_random_mute_portion\n",
    "        self.training_mode = training_mode\n",
    "        self.model, self.model_cfg = create_model(\n",
    "            self.amodel,\n",
    "            self.tmodel,\n",
    "            self.pretrained,\n",
    "            precision=\"fp32\",\n",
    "            device=\"cpu\",\n",
    "            enable_fusion=self.enable_fusion,\n",
    "            fusion_type=self.fusion_type,\n",
    "            joint_embed_shape=self.joint_embed_shape,\n",
    "        )\n",
    "\n",
    "    def get_dtype(self):\n",
    "        return next(self.model.parameters()).dtype\n",
    "    \n",
    "    def get_unconditional_condition(self, batchsize):\n",
    "        self.unconditional_token = self.model.get_text_embedding(\n",
    "            self.tokenizer([\"\", \"\"])\n",
    "        )[0:1]\n",
    "        return torch.cat([self.unconditional_token.unsqueeze(0)] * batchsize, dim=0)\n",
    "\n",
    "    def batch_to_list(self, batch):\n",
    "        ret = []\n",
    "        for i in range(batch.size(0)):\n",
    "            ret.append(batch[i])\n",
    "        return ret\n",
    "\n",
    "    def make_decision(self, probability):\n",
    "        if float(torch.rand(1)) < probability:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def random_uniform(self, start, end):\n",
    "        val = torch.rand(1).item()\n",
    "        return start + (end - start) * val\n",
    "\n",
    "    def _random_mute(self, waveform):\n",
    "        # waveform: [bs, t-steps]\n",
    "        t_steps = waveform.size(-1)\n",
    "        for i in range(waveform.size(0)):\n",
    "            mute_size = int(\n",
    "                self.random_uniform(0, end=int(t_steps * self.max_random_mute_portion))\n",
    "            )\n",
    "            mute_start = int(self.random_uniform(0, t_steps - mute_size))\n",
    "            waveform[i, mute_start : mute_start + mute_size] = 0\n",
    "        return waveform\n",
    "\n",
    "    def cos_similarity(self, waveform, text):\n",
    "        # waveform: [bs, t_steps]\n",
    "        with torch.no_grad():\n",
    "            self.embed_mode = \"audio\"\n",
    "            audio_emb = self(waveform.cuda())\n",
    "            self.embed_mode = \"text\"\n",
    "            text_emb = self(text)\n",
    "            similarity = F.cosine_similarity(audio_emb, text_emb, dim=2)\n",
    "            return similarity.squeeze()\n",
    "\n",
    "    def forward(self, batch, key=None):\n",
    "\n",
    "        # the 'fusion' truncate mode can be changed to 'rand_trunc' if run in unfusion mode\n",
    "        if self.embed_mode == \"audio\":\n",
    "            audio_dict_list = []\n",
    "            assert (\n",
    "                self.sampling_rate == 16000\n",
    "            ), \"We only support 16000 sampling rate\"\n",
    "            # batch: [bs, 1, t-samples]\n",
    "            batch = torchaudio.functional.resample(\n",
    "                batch, orig_freq=self.sampling_rate, new_freq=48000\n",
    "            )\n",
    "            \n",
    "            for waveform in self.batch_to_list(batch):\n",
    "                audio_dict = {}\n",
    "                audio_dict = get_audio_features(\n",
    "                    audio_dict,\n",
    "                    waveform.squeeze(),\n",
    "                    480000,\n",
    "                    data_truncating=\"fusion\",\n",
    "                    data_filling=\"repeatpad\",\n",
    "                    audio_cfg=self.model_cfg[\"audio_cfg\"],\n",
    "                    dtype=self.get_dtype(),\n",
    "                )\n",
    "                audio_dict_list.append(audio_dict)\n",
    "            # [bs, 768]\n",
    "            embed = self.model.get_audio_embedding(audio_dict_list)\n",
    "\n",
    "        embed = embed.unsqueeze(1)\n",
    "\n",
    "        # [bs, 1, 768]\n",
    "        return embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from contextlib import suppress\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "except ImportError:\n",
    "    wandb = None\n",
    "\n",
    "from open_clip import ClipLoss, gather_features\n",
    "from .distributed import is_master\n",
    "from .zero_shot import zero_shot_eval\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def unwrap_model(model):\n",
    "    if hasattr(model, \"module\"):\n",
    "        return model.module\n",
    "    else:\n",
    "        return model\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model, data, epoch, optimizer, scaler, scheduler, args, tb_writer=None\n",
    "):\n",
    "    device = torch.device(args.device)\n",
    "    autocast = torch.cuda.amp.autocast if args.precision == \"amp\" else suppress\n",
    "    model.train()\n",
    "    loss = ClipLoss(\n",
    "        local_loss=args.local_loss,\n",
    "        gather_with_grad=args.gather_with_grad,\n",
    "        cache_labels=True,\n",
    "        rank=args.rank,\n",
    "        world_size=args.world_size,\n",
    "        use_horovod=args.horovod,\n",
    "        mlp_loss=args.clap_mlploss,\n",
    "        weight_loss_kappa=args.kappa,\n",
    "    )\n",
    "\n",
    "    dataloader, sampler = data[\"train\"].dataloader, data[\"train\"].sampler\n",
    "    if args.distributed and sampler is not None:\n",
    "        sampler.set_epoch(epoch)\n",
    "    num_batches_per_epoch = dataloader.num_batches\n",
    "    sample_digits = math.ceil(math.log(dataloader.num_samples + 1, 10))\n",
    "\n",
    "    # for toy dataset\n",
    "    if args.dataset_type == \"toy\":\n",
    "        dataloader.dataset.generate_queue()\n",
    "\n",
    "    loss_m = AverageMeter()\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # logging.info(f\"batch {i} of {num_batches_per_epoch}\")\n",
    "        step = num_batches_per_epoch * epoch + i\n",
    "        if isinstance(scheduler, dict):\n",
    "            for s in scheduler.values():\n",
    "                s(step)\n",
    "        else:\n",
    "            scheduler(step)\n",
    "        audios = batch  # contains mel_spec, wavform, and longer list\n",
    "        texts = batch[\"text\"]\n",
    "        # audios = audios.to(device=device, non_blocking=True)\n",
    "        # texts = texts.to(device=device, non_blocking=True)\n",
    "\n",
    "        data_time_m.update(time.time() - end)\n",
    "        if isinstance(optimizer, dict):\n",
    "            for o_ in optimizer.values():\n",
    "                o_.zero_grad()\n",
    "        else:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            (\n",
    "                audio_features,\n",
    "                text_features,\n",
    "                audio_features_mlp,\n",
    "                text_features_mlp,\n",
    "                logit_scale_a,\n",
    "                logit_scale_t,\n",
    "            ) = model(audios, texts, device)\n",
    "\n",
    "            if args.clap_mlploss:\n",
    "                total_loss = loss(\n",
    "                    audio_features=audio_features,\n",
    "                    text_features=text_features,\n",
    "                    logit_scale_a=logit_scale_a,\n",
    "                    logit_scale_t=logit_scale_t,\n",
    "                    audio_features_mlp=audio_features_mlp,\n",
    "                    text_features_mlp=text_features_mlp,\n",
    "                )\n",
    "            else:\n",
    "                total_loss = loss(\n",
    "                    audio_features=audio_features,\n",
    "                    text_features=text_features,\n",
    "                    logit_scale_a=logit_scale_a,\n",
    "                )\n",
    "        if isinstance(optimizer, dict):\n",
    "            if scaler is not None:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                for o_ in optimizer.values():\n",
    "                    if args.horovod:\n",
    "                        o_.synchronize()\n",
    "                        scaler.unscale_(o_)\n",
    "                        with o_.skip_synchronize():\n",
    "                            scaler.step(o_)\n",
    "                    else:\n",
    "                        scaler.step(o_)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                for o_ in optimizer.values():\n",
    "                    o_.step()\n",
    "        else:\n",
    "            if scaler is not None:\n",
    "                scaler.scale(total_loss).backward()\n",
    "                if args.horovod:\n",
    "                    optimizer.synchronize()\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    with optimizer.skip_synchronize():\n",
    "                        scaler.step(optimizer)\n",
    "                else:\n",
    "                    scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Note: we clamp to 4.6052 = ln(100), as in the original paper.\n",
    "        with torch.no_grad():\n",
    "            unwrap_model(model).logit_scale_a.clamp_(0, math.log(100))\n",
    "            if args.clap_mlploss:\n",
    "                unwrap_model(model).logit_scale_t.clamp_(0, math.log(100))\n",
    "\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        batch_count = i + 1\n",
    "        if is_master(args) and (i % 100 == 0 or batch_count == num_batches_per_epoch):\n",
    "            if isinstance(audios, dict):\n",
    "                batch_size = len(audios[\"waveform\"])\n",
    "            else:\n",
    "                batch_size = len(audios)\n",
    "            num_samples = batch_count * batch_size * args.world_size\n",
    "            samples_per_epoch = dataloader.num_samples\n",
    "            percent_complete = 100.0 * batch_count / num_batches_per_epoch\n",
    "\n",
    "            # NOTE loss is coarsely sampled, just master node and per log update\n",
    "            loss_m.update(total_loss.item(), batch_size)\n",
    "            logit_scale_scalar_a = logit_scale_a.item()\n",
    "            logit_scale_scalar_t = logit_scale_t.item()\n",
    "            if isinstance(optimizer, dict):\n",
    "                if args.clap_mlploss:\n",
    "                    logging.info(\n",
    "                        f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                        f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                        f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                        f\"Batch (t): {batch_time_m.avg:.3f} \"\n",
    "                        f\"LR: {[o_.param_groups[0]['lr'] for o_ in optimizer.values()]} \"\n",
    "                        f\"Logit Scale Audio: {logit_scale_scalar_a:.3f}\"\n",
    "                        f\"Logit Scale Text: {logit_scale_scalar_t:.3f}\"\n",
    "                    )\n",
    "                    log_data = {\n",
    "                        \"loss\": loss_m.val,\n",
    "                        \"data_time\": data_time_m.val,\n",
    "                        \"batch_time\": batch_time_m.val,\n",
    "                        \"scale_audio\": logit_scale_scalar_a,\n",
    "                        \"scale_text\": logit_scale_scalar_t,\n",
    "                        \"lr\": [o_.param_groups[0][\"lr\"] for o_ in optimizer.values()],\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(\n",
    "                        f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                        f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                        f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                        f\"Batch (t): {batch_time_m.avg:.3f} \"\n",
    "                        f\"LR: {[o_.param_groups[0]['lr'] for o_ in optimizer.values()]} \"\n",
    "                        f\"Logit Scale Audio: {logit_scale_scalar_a:.3f}\"\n",
    "                    )\n",
    "                    log_data = {\n",
    "                        \"loss\": loss_m.val,\n",
    "                        \"data_time\": data_time_m.val,\n",
    "                        \"batch_time\": batch_time_m.val,\n",
    "                        \"scale_audio\": logit_scale_scalar_a,\n",
    "                        \"lr\": [o_.param_groups[0][\"lr\"] for o_ in optimizer.values()],\n",
    "                    }\n",
    "\n",
    "            else:\n",
    "                if args.clap_mlploss:\n",
    "                    logging.info(\n",
    "                        f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                        f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                        f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                        f\"Batch (t): {batch_time_m.avg:.3f} \"\n",
    "                        f\"LR: {optimizer.param_groups[0]['lr']:5f} \"\n",
    "                        f\"Logit Scale Audio: {logit_scale_scalar_a:.3f}\"\n",
    "                        f\"Logit Scale Text: {logit_scale_scalar_t:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # Save train loss / etc. Using non avg meter values as loggers have their own smoothing\n",
    "                    log_data = {\n",
    "                        \"loss\": loss_m.val,\n",
    "                        \"data_time\": data_time_m.val,\n",
    "                        \"batch_time\": batch_time_m.val,\n",
    "                        \"scale_audio\": logit_scale_scalar_a,\n",
    "                        \"scale_text\": logit_scale_scalar_t,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "                else:\n",
    "                    logging.info(\n",
    "                        f\"Train Epoch: {epoch} [{num_samples:>{sample_digits}}/{samples_per_epoch} ({percent_complete:.0f}%)] \"\n",
    "                        f\"Loss: {loss_m.val:#.5g} ({loss_m.avg:#.4g}) \"\n",
    "                        f\"Data (t): {data_time_m.avg:.3f} \"\n",
    "                        f\"Batch (t): {batch_time_m.avg:.3f} \"\n",
    "                        f\"LR: {optimizer.param_groups[0]['lr']:5f} \"\n",
    "                        f\"Logit Scale Audio: {logit_scale_scalar_a:.3f}\"\n",
    "                    )\n",
    "\n",
    "                    # Save train loss / etc. Using non avg meter values as loggers have their own smoothing\n",
    "                    log_data = {\n",
    "                        \"loss\": loss_m.val,\n",
    "                        \"data_time\": data_time_m.val,\n",
    "                        \"batch_time\": batch_time_m.val,\n",
    "                        \"scale_audio\": logit_scale_scalar_a,\n",
    "                        \"lr\": optimizer.param_groups[0][\"lr\"],\n",
    "                    }\n",
    "            for name, val in log_data.items():\n",
    "                name = \"train/\" + name\n",
    "                if tb_writer is not None:\n",
    "                    tb_writer.add_scalar(name, val, step)\n",
    "                if args.wandb:\n",
    "                    assert wandb is not None, \"Please install wandb.\"\n",
    "                    wandb.log({name: val, \"step\": step})\n",
    "\n",
    "            # resetting batch / data time meters per log window\n",
    "            batch_time_m.reset()\n",
    "            data_time_m.reset()\n",
    "    # end for\n",
    "\n",
    "\n",
    "def evaluate(model, data, epoch, args, tb_writer=None):\n",
    "    metrics = {}\n",
    "    if not args.parallel_eval:\n",
    "        if not is_master(args):\n",
    "            return metrics\n",
    "    device = torch.device(args.device)\n",
    "    model.eval()\n",
    "\n",
    "    # CHANGE\n",
    "    # zero_shot_metrics = zero_shot_eval(model, data, epoch, args)\n",
    "    # metrics.update(zero_shot_metrics)\n",
    "    if is_master(args):\n",
    "        print(\"Evaluating...\")\n",
    "    autocast = torch.cuda.amp.autocast if args.precision == \"amp\" else suppress\n",
    "    if args.val_dataset_names == [\"Clotho\", \"audiocaps\"]:\n",
    "        # if only clotho and audiocaps are used, then we will use a different evaluation function.\n",
    "        # This is because in the Clotho and audiocaps valid and test set, there are 5 text for 1 audio.\n",
    "        if args.parallel_eval:\n",
    "            # (yusong): just a hack here. Don't use parallel eval when evaluating only clotho and audiocaps.\n",
    "            raise NotImplementedError(\n",
    "                \"Parallel evaluation not supported for eval only Clotho and audiocaps.\"\n",
    "            )\n",
    "        val_metrics_per_dataset = evaluate_clotho_audiocaps(\n",
    "            model, data, epoch, args, autocast, device, tb_writer\n",
    "        )\n",
    "        for m in val_metrics_per_dataset.values():\n",
    "            metrics.update(m)\n",
    "        if \"epoch\" not in metrics.keys():\n",
    "            metrics.update({\"epoch\": epoch})\n",
    "        metrics = select_top_metric_clotho_audiocaps(\n",
    "            metrics, val_metrics_per_dataset, args\n",
    "        )\n",
    "    elif \"val\" in data and (\n",
    "        args.val_frequency\n",
    "        and ((epoch % args.val_frequency) == 0 or epoch == args.epochs)\n",
    "    ):\n",
    "        dataloader = data[\"val\"].dataloader\n",
    "        num_samples = 0\n",
    "        samples_per_val = dataloader.num_samples\n",
    "\n",
    "        # FIXME this does not scale past small eval datasets\n",
    "        # all_audio_features @ all_text_features will blow up memory and compute very quickly\n",
    "        eval_info = {}\n",
    "        if args.clap_mlploss:\n",
    "            eval_info[\"all\"] = {\n",
    "                \"cumulative_loss\": 0.0,\n",
    "                \"num_samples\": 0,\n",
    "                \"all_audio_features\": [],\n",
    "                \"all_text_features\": [],\n",
    "                \"all_audio_features_mlp\": [],\n",
    "                \"all_text_features_mlp\": [],\n",
    "            }  # cumulative_loss = 0.0\n",
    "        else:\n",
    "            eval_info[\"all\"] = {\n",
    "                \"cumulative_loss\": 0.0,\n",
    "                \"num_samples\": 0,\n",
    "                \"all_audio_features\": [],\n",
    "                \"all_text_features\": [],\n",
    "            }  # cumu\n",
    "        # all_audio_features, all_text_features, all_audio_features_mlp, all_text_features_mlp = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(dataloader):\n",
    "                audios = batch  # contains mel_spec, wavform, and longer list\n",
    "                texts = batch[\"text\"]\n",
    "                # audios = audios.to(device=device, non_blocking=True)\n",
    "\n",
    "                all_names = list(\n",
    "                    set([\"-\".join(b.split(\"/\")[-3:-1]) for b in batch[\"__url__\"]])\n",
    "                )\n",
    "                for name in all_names:\n",
    "                    if name not in eval_info.keys():\n",
    "                        if args.clap_mlploss:\n",
    "                            eval_info[name] = {\n",
    "                                \"cumulative_loss\": 0.0,\n",
    "                                \"num_samples\": 0,\n",
    "                                \"all_audio_features\": [],\n",
    "                                \"all_text_features\": [],\n",
    "                                \"all_audio_features_mlp\": [],\n",
    "                                \"all_text_features_mlp\": [],\n",
    "                            }\n",
    "                        else:\n",
    "                            eval_info[name] = {\n",
    "                                \"cumulative_loss\": 0.0,\n",
    "                                \"num_samples\": 0,\n",
    "                                \"all_audio_features\": [],\n",
    "                                \"all_text_features\": [],\n",
    "                            }\n",
    "                with autocast():\n",
    "                    (\n",
    "                        audio_features,\n",
    "                        text_features,\n",
    "                        audio_features_mlp,\n",
    "                        text_features_mlp,\n",
    "                        logit_scale_a,\n",
    "                        logit_scale_t,\n",
    "                    ) = model(audios, texts, device)\n",
    "\n",
    "                    if args.parallel_eval:\n",
    "                        # multi-GPU eval\n",
    "                        if args.clap_mlploss:\n",
    "                            (\n",
    "                                audio_features,\n",
    "                                text_features,\n",
    "                                audio_features_mlp,\n",
    "                                text_features_mlp,\n",
    "                            ) = gather_features(\n",
    "                                audio_features=audio_features,\n",
    "                                text_features=text_features,\n",
    "                                audio_features_mlp=audio_features_mlp,\n",
    "                                text_features_mlp=text_features_mlp,\n",
    "                                local_loss=False,\n",
    "                                gather_with_grad=False,\n",
    "                                rank=args.rank,\n",
    "                                world_size=args.world_size,\n",
    "                                use_horovod=args.horovod,\n",
    "                                mlp_loss=args.clap_mlploss,\n",
    "                            )\n",
    "                        else:\n",
    "                            (audio_features, text_features,) = gather_features(\n",
    "                                audio_features=audio_features,\n",
    "                                text_features=text_features,\n",
    "                                local_loss=False,\n",
    "                                gather_with_grad=False,\n",
    "                                rank=args.rank,\n",
    "                                world_size=args.world_size,\n",
    "                                use_horovod=args.horovod,\n",
    "                                mlp_loss=args.clap_mlploss,\n",
    "                            )\n",
    "\n",
    "                    if is_master(args):\n",
    "                        num_samples += audio_features.shape[0]\n",
    "                        for n in [*all_names, \"all\"]:\n",
    "                            if n == \"all\":\n",
    "                                eval_info[n][\"all_audio_features\"].append(\n",
    "                                    audio_features.cpu()\n",
    "                                )\n",
    "                                eval_info[n][\"all_text_features\"].append(\n",
    "                                    text_features.cpu()\n",
    "                                )\n",
    "                                if args.clap_mlploss:\n",
    "                                    eval_info[n][\"all_audio_features_mlp\"].append(\n",
    "                                        audio_features_mlp.cpu()\n",
    "                                    )\n",
    "                                    eval_info[n][\"all_text_features_mlp\"].append(\n",
    "                                        text_features_mlp.cpu()\n",
    "                                    )\n",
    "                            else:\n",
    "                                idx = np.where(\n",
    "                                    np.array(\n",
    "                                        [\n",
    "                                            \"-\".join(b.split(\"/\")[-3:-1])\n",
    "                                            for b in batch[\"__url__\"]\n",
    "                                        ]\n",
    "                                    )\n",
    "                                    == n\n",
    "                                )[0]\n",
    "                                eval_info[n][\"all_audio_features\"].append(\n",
    "                                    audio_features.cpu().index_select(\n",
    "                                        0, torch.tensor(idx).long()\n",
    "                                    )\n",
    "                                )\n",
    "                                eval_info[n][\"all_text_features\"].append(\n",
    "                                    text_features.cpu().index_select(\n",
    "                                        0, torch.tensor(idx).long()\n",
    "                                    )\n",
    "                                )\n",
    "                                if args.clap_mlploss:\n",
    "                                    eval_info[n][\"all_audio_features_mlp\"].append(\n",
    "                                        audio_features_mlp.cpu().index_select(\n",
    "                                            0, torch.tensor(idx).long()\n",
    "                                        )\n",
    "                                    )\n",
    "                                    eval_info[n][\"all_text_features_mlp\"].append(\n",
    "                                        text_features_mlp.cpu().index_select(\n",
    "                                            0, torch.tensor(idx).long()\n",
    "                                        )\n",
    "                                    )\n",
    "                        #  print(f'eval step {i}') #  (yusong): for debug\n",
    "\n",
    "                # cumulative_loss += total_loss * batch_size\n",
    "                # num_samples += batch_size\n",
    "                if is_master(args) and (i % 100) == 0:  # and i != 0:\n",
    "                    logging.info(\n",
    "                        f\"Eval Epoch: {epoch} [{num_samples} / {samples_per_val}]\"\n",
    "                    )\n",
    "            if is_master(args):\n",
    "                val_metrics_per_dataset = {}\n",
    "                for n in eval_info.keys():\n",
    "                    if args.clap_mlploss:\n",
    "                        metrics_single_dataset = get_metrics(\n",
    "                            audio_features=torch.cat(\n",
    "                                eval_info[n][\"all_audio_features\"]\n",
    "                            ),\n",
    "                            text_features=torch.cat(eval_info[n][\"all_text_features\"]),\n",
    "                            logit_scale_a=logit_scale_a.cpu(),\n",
    "                            audio_features_mlp=torch.cat(\n",
    "                                eval_info[n][\"all_audio_features_mlp\"]\n",
    "                            ),\n",
    "                            text_features_mlp=torch.cat(\n",
    "                                eval_info[n][\"all_text_features_mlp\"]\n",
    "                            ),\n",
    "                            logit_scale_t=logit_scale_t.cpu(),\n",
    "                            mlp_loss=args.clap_mlploss,\n",
    "                        )\n",
    "                    else:\n",
    "                        metrics_single_dataset = get_metrics(\n",
    "                            audio_features=torch.cat(\n",
    "                                eval_info[n][\"all_audio_features\"]\n",
    "                            ),\n",
    "                            text_features=torch.cat(eval_info[n][\"all_text_features\"]),\n",
    "                            logit_scale_a=logit_scale_a.cpu(),\n",
    "                            mlp_loss=args.clap_mlploss,\n",
    "                        )\n",
    "                    val_metrics_per_dataset[n] = {\n",
    "                        n + \"/\" + k: v for k, v in metrics_single_dataset.items()\n",
    "                    }\n",
    "                    metrics.update(val_metrics_per_dataset[n])\n",
    "                    if \"epoch\" not in metrics.keys():\n",
    "                        metrics.update({\"epoch\": epoch})\n",
    "    if is_master(args):\n",
    "        if not metrics:\n",
    "            return metrics\n",
    "\n",
    "        logging.info(\n",
    "            f\"Eval Epoch: {epoch} \"\n",
    "            + \"\\n\".join(\n",
    "                [\n",
    "                    \"\\t\".join([f\"{k}: {round(v, 4):.4f}\" for k, v in m.items()])\n",
    "                    for m in val_metrics_per_dataset.values()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if args.save_logs:\n",
    "            for name, val in metrics.items():\n",
    "                if tb_writer is not None:\n",
    "                    tb_writer.add_scalar(f\"val/{name}\", val, epoch)\n",
    "\n",
    "            with open(os.path.join(args.checkpoint_path, \"results.jsonl\"), \"a+\") as f:\n",
    "                f.write(json.dumps(metrics))\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "        if args.wandb:\n",
    "            assert wandb is not None, \"Please install wandb.\"\n",
    "            for name, val in metrics.items():\n",
    "                wandb.log({f\"val/{name}\": val, \"epoch\": epoch})\n",
    "\n",
    "        return metrics\n",
    "    else:\n",
    "        return metrics\n",
    "\n",
    "\n",
    "def get_metrics(\n",
    "    audio_features,\n",
    "    text_features,\n",
    "    logit_scale_a,\n",
    "    audio_features_mlp=None,\n",
    "    text_features_mlp=None,\n",
    "    logit_scale_t=None,\n",
    "    mlp_loss=False,\n",
    "):\n",
    "    metrics = {}\n",
    "    if mlp_loss:\n",
    "        # Set up audio to text & text to audio similary matrice\n",
    "        a_logits_per_audio = (\n",
    "            (logit_scale_a * audio_features @ text_features_mlp.t()).detach().cpu()\n",
    "        )\n",
    "        a_logits_per_text = a_logits_per_audio.t().detach().cpu()\n",
    "        t_logits_per_audio = (\n",
    "            (logit_scale_t * audio_features_mlp @ text_features.t()).detach().cpu()\n",
    "        )\n",
    "        t_logits_per_text = t_logits_per_audio.t().detach().cpu()\n",
    "\n",
    "        labels = torch.arange(audio_features.shape[0]).long()\n",
    "        # Change the loss from two terms into four terms with 2x2 combined CE loss\n",
    "        total_loss = (\n",
    "            F.cross_entropy(a_logits_per_audio, labels)\n",
    "            + F.cross_entropy(a_logits_per_text, labels)\n",
    "            + F.cross_entropy(t_logits_per_audio, labels)\n",
    "            + F.cross_entropy(t_logits_per_text, labels)\n",
    "        ) / 4\n",
    "\n",
    "        metrics[f\"cumulative_loss\"] = total_loss.item()\n",
    "        metrics[f\"num_samples\"] = audio_features.shape[0]\n",
    "\n",
    "        logits = {\n",
    "            \"audio_to_text\": (a_logits_per_audio + t_logits_per_audio) / 2,\n",
    "            \"text_to_audio\": (a_logits_per_text + t_logits_per_text) / 2,\n",
    "        }\n",
    "        ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    else:\n",
    "        # print(\"text_features\", text_features)\n",
    "        # print(\"text_features.shape\", text_features.shape)\n",
    "        logits_per_audio = (\n",
    "            (logit_scale_a * audio_features @ text_features.t()).detach().cpu()\n",
    "        )\n",
    "        logits_per_text = logits_per_audio.t().detach().cpu()\n",
    "\n",
    "        labels = torch.arange(audio_features.shape[0]).long()\n",
    "        # Change the loss from two terms into four terms with 2x2 combined CE loss\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_audio, labels)\n",
    "            + F.cross_entropy(logits_per_text, labels)\n",
    "        ) / 2\n",
    "\n",
    "        metrics[f\"cumulative_loss\"] = total_loss.item()\n",
    "        metrics[f\"num_samples\"] = audio_features.shape[0]\n",
    "\n",
    "        logits = {\"audio_to_text\": logits_per_audio, \"text_to_audio\": logits_per_text}\n",
    "\n",
    "        ground_truth = torch.arange(len(text_features)).view(-1, 1)\n",
    "\n",
    "    for name, logit in logits.items():\n",
    "        ranking = torch.argsort(logit, descending=True)\n",
    "        preds = torch.where(ranking == ground_truth)[\n",
    "            1\n",
    "        ]  # (yusong) this line is slow because it uses single thread\n",
    "        preds = preds.detach().cpu().numpy()\n",
    "        metrics[f\"{name}_mean_rank\"] = preds.mean() + 1\n",
    "        metrics[f\"{name}_median_rank\"] = np.floor(np.median(preds)) + 1\n",
    "        for k in [1, 5, 10]:\n",
    "            metrics[f\"{name}_R@{k}\"] = np.mean(preds < k)\n",
    "        # map@10\n",
    "        metrics[f\"{name}_mAP@10\"] = np.mean(np.where(preds < 10, 1 / (preds + 1), 0.0))\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def evaluate_clotho_audiocaps(\n",
    "    model, data, epoch, args, autocast, device, tb_writer=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/XinhaoMei/audio-text_retrieval/blob/main/tools/utils.py.\n",
    "    1. for text-to-audio retrieval, do 5 times and average the results\n",
    "    2. for R@1, R@5, R@10 in audio-to-text retrieval, take the best rank among 5 text\n",
    "    3. for map@10 in audio-to-text retrieval:\n",
    "        3.1: sort the rank of 5 text\n",
    "        3.2: exclude the rank >=10 (0-index)\n",
    "        3.3: compute the map regarding the remaining ranks: np.mean(np.arange(1, len(ranks)+1) / ranks).\n",
    "        (3.3) That is, take the top ranks of 5 text that is < 10, and assign the descending number as ground truth.\n",
    "        (3.3) E.g.: the ground truth of first rank of the 5 text should be 1, the second rank should be 2, etc.\n",
    "    \"\"\"\n",
    "    # TODO: (yusong) only support single GPU evaluation and only support non-mlp case for now.\n",
    "    dataloader = data[\"val\"].dataloader\n",
    "    with torch.no_grad():\n",
    "        eval_info = {}\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            audios = batch  # contains mel_spec, wavform, and longer list\n",
    "\n",
    "            # each item in the list has 5 texts\n",
    "            if args.tmodel == \"transformer\":\n",
    "                from open_clip import tokenize\n",
    "\n",
    "                texts = [tokenize(t) for t in batch[\"full_text\"]]\n",
    "                texts = torch.cat(texts)\n",
    "            else:\n",
    "                from .data import tokenizer\n",
    "\n",
    "                texts = [\n",
    "                    tokenizer(t) for t in batch[\"full_text\"]\n",
    "                ]  # 5 texts for each audio\n",
    "                texts = {\n",
    "                    k: torch.cat([t[k] for t in texts]) for k in texts[0].keys()\n",
    "                }  # 5 x batch\n",
    "\n",
    "            # audios = audios.to(device=device, non_blocking=True)\n",
    "\n",
    "            all_names = list(\n",
    "                set([\"-\".join(b.split(\"/\")[-3:-1]) for b in batch[\"__url__\"]])\n",
    "            )\n",
    "            for name in all_names:\n",
    "                if name not in eval_info.keys():\n",
    "                    # we will not use mlp outputs even if args.clap_mlploss=True\n",
    "                    eval_info[name] = {\n",
    "                        \"cumulative_loss\": 0.0,\n",
    "                        \"num_samples\": 0,\n",
    "                        \"all_audio_features\": [],\n",
    "                        \"all_text_features\": [],\n",
    "                    }\n",
    "            with autocast():\n",
    "                audio_features = model(audios, None, device)\n",
    "                text_features = model(None, texts, device)\n",
    "                audio_features = F.normalize(audio_features, dim=-1)\n",
    "                text_features = F.normalize(text_features, dim=-1)\n",
    "\n",
    "                all_names = list(\n",
    "                    set([\"-\".join(b.split(\"/\")[-3:-1]) for b in batch[\"__url__\"]])\n",
    "                )\n",
    "                for n in all_names:\n",
    "                    idx = np.where(\n",
    "                        np.array(\n",
    "                            [\"-\".join(b.split(\"/\")[-3:-1]) for b in batch[\"__url__\"]]\n",
    "                        )\n",
    "                        == n\n",
    "                    )[0]\n",
    "                    eval_info[n][\"all_audio_features\"].append(\n",
    "                        audio_features.cpu().index_select(0, torch.tensor(idx).long())\n",
    "                    )\n",
    "                    # (yusong) please double-check. This is for selecting 5 text features at once.\n",
    "                    # because idx is a list of indices in size of num_samples,\n",
    "                    # and text_features is a tensor of size (5*num_samples, dim)\n",
    "                    # so we need to select 5 consecutive indices at once for a single index in idx.\n",
    "                    eval_info[n][\"all_text_features\"].append(\n",
    "                        text_features.cpu()\n",
    "                        .reshape([-1, 5, text_features.shape[1]])\n",
    "                        .index_select(0, torch.tensor(idx).long())\n",
    "                        .reshape([-1, text_features.shape[1]])\n",
    "                    )\n",
    "\n",
    "        val_metrics_all = {}\n",
    "\n",
    "        for n in eval_info.keys():\n",
    "            logit_scale_a, logit_scale_t = model(None, None, device)\n",
    "            logit_scale_a = logit_scale_a.cpu()\n",
    "\n",
    "            audio_features = torch.cat(eval_info[n][\"all_audio_features\"], dim=0)\n",
    "            text_features = torch.cat(eval_info[n][\"all_text_features\"], dim=0)\n",
    "\n",
    "            logits_per_audio = (\n",
    "                (logit_scale_a * audio_features @ text_features.t()).detach().cpu()\n",
    "            )\n",
    "            logits_per_text = logits_per_audio.t().detach().cpu()\n",
    "\n",
    "            # logits_per_audio shape: [num_samples, num_samples*5]\n",
    "            # logits_per_text shape: [num_samples*5, num_samples]\n",
    "\n",
    "            logging.info(\n",
    "                f\"dataset {n}, logits_per_audio shape: {logits_per_audio.shape}, \"\n",
    "                f\"logits_per_text shape: {logits_per_text.shape}\"\n",
    "            )\n",
    "\n",
    "            metrics = {}\n",
    "            num_samples = audio_features.shape[0]\n",
    "            metrics[f\"num_samples\"] = num_samples\n",
    "\n",
    "            # (yusong) the following code is very important, please double-check:\n",
    "            # logits_per_audio.reshape(num_samples, num_samples, 5)[:, :, d]\n",
    "            # logits_per_text.reshape(num_samples, 5, num_samples)[:, d, :]\n",
    "            # Those two are retrieving one of the 5 text for each audio.\n",
    "            labels = torch.arange(audio_features.shape[0]).long()\n",
    "            audio_to_text_loss = [\n",
    "                F.cross_entropy(\n",
    "                    logits_per_audio.reshape(num_samples, num_samples, 5)[:, :, d],\n",
    "                    labels,\n",
    "                )\n",
    "                for d in range(5)\n",
    "            ]\n",
    "            text_to_audio_loss = [\n",
    "                F.cross_entropy(\n",
    "                    logits_per_text.reshape(num_samples, 5, num_samples)[:, d, :],\n",
    "                    labels,\n",
    "                )\n",
    "                for d in range(5)\n",
    "            ]\n",
    "            total_loss = (np.mean(audio_to_text_loss) + np.mean(text_to_audio_loss)) / 2\n",
    "\n",
    "            metrics[f\"cumulative_loss\"] = total_loss.item()\n",
    "\n",
    "            # text to audio: do 5 times\n",
    "            pred_text = []\n",
    "            for d in range(5):\n",
    "                logit = logits_per_text.reshape(num_samples, 5, num_samples)[:, d, :]\n",
    "                ground_truth = torch.arange(len(logit)).view(-1, 1)\n",
    "                ranking = torch.argsort(\n",
    "                    logit, descending=True\n",
    "                )  # [num_samples, num_samples]\n",
    "                preds = torch.where(ranking == ground_truth)[1]\n",
    "                pred_text.append(preds.detach().cpu().numpy())\n",
    "            pred_text_concat = np.concatenate(pred_text, axis=0)  # [5*num_samples]\n",
    "            metrics[f\"text_to_audio_mean_rank\"] = pred_text_concat.mean() + 1\n",
    "            metrics[f\"text_to_audio_median_rank\"] = (\n",
    "                np.floor(np.median(pred_text_concat)) + 1\n",
    "            )\n",
    "            for k in [1, 5, 10]:\n",
    "                metrics[f\"text_to_audio_R@{k}\"] = np.mean(pred_text_concat < k)\n",
    "            # map@10\n",
    "            metrics[f\"text_to_audio_mAP@10\"] = np.mean(\n",
    "                np.where(pred_text_concat < 10, 1 / (pred_text_concat + 1), 0.0)\n",
    "            )\n",
    "\n",
    "            # audio to text: take the best result\n",
    "            # for audio to text map 10, sort and assign descending ground truth.\n",
    "            # see https://github.com/XinhaoMei/audio-text_retrieval/blob/main/tools/utils.py#L103\n",
    "            # map@10\n",
    "            map_all = []\n",
    "            pred_audio_all = []\n",
    "            for d in range(num_samples):\n",
    "                # logits_per_audio: [num_samples, num_samples*5]\n",
    "                logit_single = logits_per_audio[d, :]  # [5*num_samples]\n",
    "                # Ground-truth index: [d*5, d*5+1, d*5+2, d*5+3, d*5+4]\n",
    "                ranking = torch.argsort(\n",
    "                    logit_single, descending=True\n",
    "                )  # [5*num_samples]\n",
    "                # ranking: the index of first match, second match, ...\n",
    "                ground_truth = torch.arange(d * 5, d * 5 + 5)[None]\n",
    "                all_pred = torch.where(\n",
    "                    torch.stack([ranking] * 5) == ground_truth.view(-1, 1)\n",
    "                )[1]\n",
    "                min_pred = torch.min(all_pred)\n",
    "                pred_audio_all.append(min_pred.detach().cpu().numpy())\n",
    "                all_pred_filter = all_pred[all_pred < 10].detach().cpu().numpy()\n",
    "                # /5 because we have 5 text, so it means for the text rank >=10 we count as 0.\n",
    "                map_single = (\n",
    "                    np.sum(\n",
    "                        (np.arange(1, len(all_pred_filter) + 1) / (all_pred_filter + 1))\n",
    "                    )\n",
    "                    / 5\n",
    "                )\n",
    "                map_all.append(map_single)\n",
    "            metrics[f\"audio_to_text_mAP@10\"] = np.mean(map_all)\n",
    "            for k in [1, 5, 10]:\n",
    "                metrics[f\"audio_to_text_R@{k}\"] = np.mean(np.array(pred_audio_all) < k)\n",
    "\n",
    "            val_metrics_all[n] = {n + \"/\" + k: v for k, v in metrics.items()}\n",
    "    return val_metrics_all\n",
    "\n",
    "\n",
    "def calculate_selection_performance_clotho_audiocaps(val_metrics_per_dataset):\n",
    "    \"\"\"\n",
    "    Calculate performance for Clotho+AudioCaps for model selection.\n",
    "    \"\"\"\n",
    "    selection_performance_all = []\n",
    "    for n in val_metrics_per_dataset.keys():\n",
    "        selection_performance = (\n",
    "            val_metrics_per_dataset[n][f\"{n}/audio_to_text_mAP@10\"]\n",
    "            + val_metrics_per_dataset[n][f\"{n}/text_to_audio_mAP@10\"]\n",
    "        ) / 2\n",
    "        selection_performance_all.append(selection_performance)\n",
    "    return np.mean(selection_performance_all)\n",
    "\n",
    "\n",
    "def select_top_metric_clotho_audiocaps(metrics, val_metrics_per_dataset, args):\n",
    "    # val_metrics_per_dataset: dict, key: dataset name, value: dict, key: metric name, value: metric value\n",
    "    # metrics: dict, key: metric name, value: metric value\n",
    "    # Hack: use args to save the top performance\n",
    "    if not hasattr(args, \"top_selection_performance\"):\n",
    "        selection_performance = calculate_selection_performance_clotho_audiocaps(\n",
    "            val_metrics_per_dataset\n",
    "        )\n",
    "        # TODO: write the if and else together\n",
    "        metric_update = {}\n",
    "        for n in val_metrics_per_dataset.keys():\n",
    "            for k in val_metrics_per_dataset[n].keys():\n",
    "                metric_update[\n",
    "                    k.split(\"/\")[0] + \"-top\" + \"/\" + k.split(\"/\")[1]\n",
    "                ] = val_metrics_per_dataset[n][k]\n",
    "        metric_update[\"top_selection_performance\"] = selection_performance\n",
    "        metric_update[\"top-selection-epoch\"] = metrics[\"epoch\"]\n",
    "        metrics.update(metric_update)\n",
    "        args.top_metric = metric_update\n",
    "        args.top_selection_performance = selection_performance\n",
    "    else:\n",
    "        selection_performance_new = calculate_selection_performance_clotho_audiocaps(\n",
    "            val_metrics_per_dataset\n",
    "        )\n",
    "        selection_performance_old = args.top_selection_performance\n",
    "        if selection_performance_new > selection_performance_old:\n",
    "            metric_update = {}\n",
    "            for n in val_metrics_per_dataset.keys():\n",
    "                for k in val_metrics_per_dataset[n].keys():\n",
    "                    metric_update[\n",
    "                        k.split(\"/\")[0] + \"-top\" + \"/\" + k.split(\"/\")[1]\n",
    "                    ] = val_metrics_per_dataset[n][k]\n",
    "            metric_update[\"top_selection_performance\"] = selection_performance_new\n",
    "            metric_update[\"top-selection-epoch\"] = metrics[\"epoch\"]\n",
    "            metrics.update(metric_update)\n",
    "            args.top_metric = metric_update\n",
    "            args.top_selection_performance = selection_performance_new\n",
    "        else:\n",
    "            metrics.update(args.top_metric)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### infer-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Pretrained weights (clap_modules/open_clip/model_configs/htsat-tyny.json) not found for model HTSAT-tiny.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Pretrained weights (clap_modules/open_clip/model_configs/htsat-tyny.json) not found for model HTSAT-tiny.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 107\u001b[0m\n\u001b[1;32m    103\u001b[0m     ipdb\u001b[38;5;241m.\u001b[39mset_trace()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[43minfer_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     infer_audio()\n",
      "Cell \u001b[0;32mIn[20], line 41\u001b[0m, in \u001b[0;36minfer_text\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m fusion_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maff_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m pretrained \u001b[38;5;241m=\u001b[39m PRETRAINED_PATH\n\u001b[0;32m---> 41\u001b[0m model, model_cfg \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprecision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_fusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_fusion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfusion_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfusion_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# load the text, can be a list (i.e. batch size)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m text_data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love the contrastive learning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love the pretrain model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/raid/m236866/md-mt/core/models/encoders/clap_modules/open_clip/factory.py:157\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(amodel_name, tmodel_name, pretrained, precision, device, jit, force_quick_gelu, openai_model_cache_dir, skip_params, pretrained_audio, pretrained_text, enable_fusion, fusion_type, joint_embed_shape)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;66;03m# for n in param_names:\u001b[39;00m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;66;03m#     print(n, \"\\t\", \"Loaded\" if n in ckpt else \"Unloaded\")\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m         logging\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    155\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrained weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not found for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         )\n\u001b[0;32m--> 157\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    158\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPretrained weights (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) not found for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mamodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pretrained_audio:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m amodel_name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPANN\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Pretrained weights (clap_modules/open_clip/model_configs/htsat-tyny.json) not found for model HTSAT-tiny."
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import librosa\n",
    "from clap_modules.open_clip import create_model\n",
    "from clap_modules.training.data import get_audio_features\n",
    "from clap_modules.training.data import int16_to_float32, float32_to_int16\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenize = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    result = tokenize(\n",
    "        text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    return {k: v.squeeze(0) for k, v in result.items()}\n",
    "\n",
    "\n",
    "#PRETRAINED_PATH = \"/mnt/fast/nobackup/users/hl01486/projects/contrastive_pretraining/CLAP/assets/checkpoints/epoch_top_0_audioset_no_fusion.pt\"\n",
    "\n",
    "PRETRAINED_PATH = \"clap_modules/open_clip/model_configs/HTSAT-tyny.json\"\n",
    "\n",
    "WAVE_48k_PATH = \"/mnt/fast/nobackup/users/hl01486/projects/contrastive_pretraining/CLAP/assets/audio/machine.wav\"\n",
    "\n",
    "\n",
    "def infer_text():\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    precision = \"fp32\"\n",
    "    amodel = \"HTSAT-tiny\"  # or 'PANN-14'\n",
    "    tmodel = \"roberta\"  # the best text encoder in our training\n",
    "    enable_fusion = False  # False if you do not want to use the fusion model\n",
    "    fusion_type = \"aff_2d\"\n",
    "    pretrained = PRETRAINED_PATH\n",
    "\n",
    "    model, model_cfg = create_model(\n",
    "        amodel,\n",
    "        tmodel,\n",
    "        pretrained,\n",
    "        precision=precision,\n",
    "        device=device,\n",
    "        enable_fusion=enable_fusion,\n",
    "        fusion_type=fusion_type,\n",
    "    )\n",
    "    # load the text, can be a list (i.e. batch size)\n",
    "    text_data = [\"I love the contrastive learning\", \"I love the pretrain model\"]\n",
    "    # tokenize for roberta, if you want to tokenize for another text encoder, please refer to data.py#L43-90\n",
    "    text_data = tokenizer(text_data)\n",
    "\n",
    "    text_embed = model.get_text_embedding(text_data)\n",
    "    print(text_embed.size())\n",
    "\n",
    "\n",
    "def infer_audio():\n",
    "\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    precision = \"fp32\"\n",
    "    amodel = \"HTSAT-tiny\"  # or 'PANN-14'\n",
    "    tmodel = \"roberta\"  # the best text encoder in our training\n",
    "    enable_fusion = False  # False if you do not want to use the fusion model\n",
    "    fusion_type = \"aff_2d\"\n",
    "    pretrained = PRETRAINED_PATH\n",
    "\n",
    "    model, model_cfg = create_model(\n",
    "        amodel,\n",
    "        tmodel,\n",
    "        pretrained,\n",
    "        precision=precision,\n",
    "        device=device,\n",
    "        enable_fusion=enable_fusion,\n",
    "        fusion_type=fusion_type,\n",
    "    )\n",
    "\n",
    "    # load the waveform of the shape (T,), should resample to 48000\n",
    "    audio_waveform, sr = librosa.load(WAVE_48k_PATH, sr=48000)\n",
    "    # quantize\n",
    "    audio_waveform = int16_to_float32(float32_to_int16(audio_waveform))\n",
    "    audio_waveform = torch.from_numpy(audio_waveform).float()\n",
    "    audio_dict = {}\n",
    "\n",
    "    # the 'fusion' truncate mode can be changed to 'rand_trunc' if run in unfusion mode\n",
    "    import ipdb\n",
    "\n",
    "    ipdb.set_trace()\n",
    "    audio_dict = get_audio_features(\n",
    "        audio_dict,\n",
    "        audio_waveform,\n",
    "        480000,\n",
    "        data_truncating=\"fusion\",\n",
    "        data_filling=\"repeatpad\",\n",
    "        audio_cfg=model_cfg[\"audio_cfg\"],\n",
    "    )\n",
    "    # can send a list to the model, to process many audio tracks in one time (i.e. batch size)\n",
    "    audio_embed = model.get_audio_embedding([audio_dict])\n",
    "    print(audio_embed.size())\n",
    "    import ipdb\n",
    "\n",
    "    ipdb.set_trace()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    infer_text()\n",
    "    infer_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
